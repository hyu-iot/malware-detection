import numpy as np
import pandas as pd
import torch
from torch.utils.data import TensorDataset, DataLoader

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

import os


conn_state_dict = {
    'S0': 0,
    'S1': 1,
    'S2': 2,
    'S3': 3,
    'SF': 4,
    'SH': 5,
    'SHR': 6,
    'REJ': 7,
    'RSTO': 8,
    'RSTR': 9,
    'RSTOS0': 10,
    'RSTRH': 11,
    'OTH': 12,
}

proto_dict = {
    'tcp': 0,
    'udp': 1,
    'icmp': 2,
}

service_dict = {
    ' ': 0,
    'http': 1,
    'dhcp': 2,
    'ssh': 3,
    'dns': 4,
    'irc': 5,
    'ssl': 6,
}

replace_dicts = {
    "proto": proto_dict,
    "service": service_dict,
    "conn-state": conn_state_dict,
}

all_labels = ['Attack', 'Benign', 'C&C', 'C&C-FileDownload', 'C&C-HeartBeat', 
          'C&C-HeartBeat-Attack', 'C&C-HeartBeat-FileDownload', 'C&C-Mirai', 
          'C&C-PartOfAHorizontalPortScan', 'C&C-Torii', 'DDoS', 'FileDownload', 
          'Okiru', 'PartOfAHorizontalPortScan', 'PartOfAHorizontalPortScan-Attack']


def convert_history_to_numeric(history):
  if history == ' ':
    return 0
  
  if isinstance(history, int):
    return history

  letters = ['s', 'h', 'a', 'd', 'f', 'r', 'c', 'g', 't', 'w', 'i', 'q', '^',
             'S', 'H', 'A', 'D', 'F', 'R', 'C', 'G', 'T', 'W', 'I', 'Q']
  
  history_dict = dict(zip(letters, range(1, len(letters)+1)))
  result = 1

  for i, c in enumerate(history):
    if i >= 5:  break
    result = result * len(history) + history_dict[c]

  return result


class ClientManager():
  def __init__(self, config):
    self.config = config

    files = [f for f in os.listdir(config['DIR_PATH']) if os.path.isfile(os.path.join(config['DIR_PATH'], f))]
    self.num_total_files = len(files)
    
  # Return Round object of target round
  def round(self, idx):
    
    # Get files that will be used by target round
    num_files = int(self.config['period'] / self.config['unit_period'])
    start_idx, end_idx = idx*num_files, min( (idx+1)*num_files-1, self.num_total_files-1)
    filenames = [f"{self.config['device']}_{self.config['unit_period']}_{i}.csv" for i in range(start_idx, end_idx+1)]
    
    # concatenate the files
    #print(f"Device {idx}: start to concatenate the file")
    
    if len(filenames) == 0:
      print(f"Device {self.config['device']} has no file at Round {idx}")
      return Round(None, None, None, None, None)

    columns = pd.read_csv(os.path.join(self.config['DIR_PATH'], filenames[0])).columns
    df_round = pd.DataFrame(columns=columns)
    for filename in filenames:
      df_file = pd.read_csv(os.path.join(self.config['DIR_PATH'], filename))
      df_round = pd.concat([df_round, df_file])
    #print(f"Device {idx}: end to concatenate the file")      

    if len(df_round) < 2:
      return Round(df_round, None, None, None, None)

    # Preprocess data
    #print(f"Device {idx}: data preprocessing start")
    df_X, df_label = self.__preprocess_dataframe(df_round)
    #print(f"Device {idx}: data preprocessing end")
    

    # Make Dataset and Dataloader
    trainset, testset, trainloader, testloader = self.__make_train_and_test_data(df_X, df_label)
    
    return Round(df_round, trainset, testset, trainloader, testloader)

  def __preprocess_dataframe(self, df):


    def swap(label, p):
      rand_num = np.random.random()
      swap_dict = {
          'Benign': 'Malicious',
          'Malicious': 'Benign',
      }
      return swap_dict[label] if rand_num <= p else label



    df = df.drop(columns=['ts', 'id.orig_h', 'id.resp_h'])
    df = df.replace(replace_dicts)
    df.history = df.history.apply(convert_history_to_numeric)


    df.loc[df.label != 'Benign', 'label'] = 'Malicious'
    df.label = df.label.apply(lambda x: swap(x, 1-self.config['reliability']))
    df_labels = pd.get_dummies(df.label)
    df = df.drop(columns=['label'])



    # label_columns = ['label_' + label for label in all_labels]

    # df_current_labels = pd.get_dummies(df, columns=['label'])
    # df_labels = pd.DataFrame(columns=label_columns)

    # for label in label_columns:
    #   if label in df_current_labels.columns.values:
    #     df_labels[label] = df_current_labels[label].copy()

    # df_labels = df_labels.fillna(0)
    # df = df.drop(columns=['label'])  
    

    df = df.astype(float)
    df_labels = df_labels.astype(int)

    scaler = MinMaxScaler()
    scaler.fit(df)
    df = pd.DataFrame(scaler.transform(df), columns=df.columns)


    return df, df_labels

  def __make_train_and_test_data(self, df_X, df_y):
    
    # Split train and test data
    X_train, X_test, y_train, y_test = train_test_split(
        df_X, 
        df_y, 
        train_size=self.config['train_ratio'],
        random_state=self.config['random_seed'],
    )

    X_train, y_train = torch.tensor(X_train.values), torch.tensor(y_train.values)
    X_test, y_test = torch.tensor(X_test.values), torch.tensor(y_test.values)

    trainset, testset = TensorDataset(X_train, y_train), TensorDataset(X_test, y_test)

    trainloader = DataLoader(
        trainset,
        batch_size=self.config['batch_size'],
        shuffle=self.config['shuffle'],
    )

    testloader = DataLoader(
        testset,
        batch_size=self.config['batch_size'],
        shuffle=self.config['shuffle'],
    )

    return trainset, testset, trainloader, testloader

  
class Round():
  def __init__(self, data, trainset, testset, trainloader, testloader):
    self.data = data
    self.trainset = trainset
    self.testset = testset
    self.trainloader = trainloader
    self.testloader = testloader

  def get_dataset(self):
    return self.trainset, self.testset

  def get_dataloader(self):
    return self.trainloader, self.testloader